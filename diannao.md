# 大电脑
[![DianNao.pdf](https://cldup.com/dTxpPi9lDf.thumb.png)](http://novel.ict.ac.cn/ychen/pdf/DianNao.pdf)

## 摘要
机器学习任务在广泛的领域和广泛的系统（从嵌入式系统到数据中心）中变得越来越普遍。同时，一些机器学习算法（特别是卷积和深度神经网络，即CNNs和DNNs）在许多应用中被证明是最先进的。随着体系结构朝着由核心和加速器混合组成的异构多核的发展，机器学习加速器可以实现极少算法组合（由于目标算法的数量较少）和广泛的应用范围的有效结合。到目前为止，大多数机器学习加速器设计都集中在有效地实现算法的计算部分。然而，近期最先进的CNN和DNN的都具备大规模的特点。在这项研究中，我们设计了一个加速器大型CNN和DNN，特别强调的内存对加速器设计的性能和功耗的影响。我们利用65nm工艺，设计了一个具有高吞吐量的加速器，能够执行452个GOP/S（关键神经网络操作，例如突触权重乘法和神经元输出加法），而且芯片面积可以小至3.02mm2，功耗为485mW。与128位2GHz SIMD处理器相比， 加速器速度快1178x， 能将总能耗降低21.08x。这样的高吞吐量小面积设计使得最先进的机器学习算法可以被应用在广泛的系统和应用场景中。

#介绍

随着体系结构向异构、多核和加速的混合，实现灵活性和效率之间的最佳折衷成为设计加速器的突出问题。
第一个问题是应该主要针对哪一类应用来设计加速器？随着体系结构朝着加速器的方向发展的趋势，高性能嵌入式应用的第二个同时也是显著的趋势正在发展：许多新兴的高性能和嵌入式应用，从图像/视频/音频识别到自动翻译和商业分析，以及所有形式的机器人都依赖于机器学习技术。这种趋势甚至开始渗透到我们的社区中，在那里，一个用于评价新应用的套件，PARSEC[2]，其中有一般的评分是采用了机器学习算法[4]。这种应用的趋势伴随着机器学习的第三个显著的趋势，其中基于神经网络（特别是卷积神经网络[27]和深度神经网络[16]〕的少量技术在过去几年中脱颖而出。因此，加速器设计迎来了一个独特的机会，可以实现两个方面的最佳：广泛的应用范围，以及高性能和高效率，主要原因是目标算法只有有限个。
目前，这些工作大多在GPU[5]，或在FPGA[3]  上通过SIMD[41]执行。 然而，有些研究人员已经意识到了上述的趋势，并且提出了用于CNN[3]或MLP[38]的加速器设计；在其他应用领域方面，如图像处理，也提出了针对机器学习原语的高效实现，如卷积。还有一些人提出了CNN和定制机器学习算法的ASIC实现。然而，所有这些工作都只是专注于这些计算原语的高效实现，却为了简单起见，有意的回避内存传输，或者直接把计算加速器通过一个简单的DMA，挂载在内存上。
