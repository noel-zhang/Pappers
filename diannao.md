# 大电脑
[![DianNao.pdf](https://cldup.com/dTxpPi9lDf.thumb.png)](http://novel.ict.ac.cn/ychen/pdf/DianNao.pdf)

## 摘要
机器学习任务在广泛的领域和广泛的系统（从嵌入式系统到数据中心）中变得越来越普遍。同时，一些机器学习算法（特别是卷积和深度神经网络，即CNNs和DNNs）在许多应用中被证明是最先进的。随着体系结构朝着由核心和加速器混合组成的异构多核的发展，机器学习加速器可以实现极少算法组合（由于目标算法的数量较少）和广泛的应用范围的有效结合。到目前为止，大多数机器学习加速器设计都集中在有效地实现算法的计算部分。然而，近期最先进的CNN和DNN的都具备大规模的特点。在这项研究中，我们设计了一个加速器大型CNN和DNN，特别强调的内存对加速器设计的性能和功耗的影响。我们利用65nm工艺，设计了一个具有高吞吐量的加速器，能够执行452个GOP/S（关键神经网络操作，例如突触权重乘法和神经元输出加法），而且芯片面积可以小至3.02mm2，功耗为485mW。与128位2GHz SIMD处理器相比， 加速器速度快1178x， 能将总能耗降低21.08x。这样的高吞吐量小面积设计使得最先进的机器学习算法可以被应用在广泛的系统和应用场景中。

## 1. 介绍

随着体系结构向异构多核，混合了多核心和加速器方向演化，实现灵活性和效率之间的最佳折衷成为加速器设计的突出问题。
第一个问题是应该主要针对哪一类应用来设计加速器？随着体系结构朝着加速器的方向发展的趋势，高性能嵌入式应用的第二个同时也是显著的趋势正在发展：许多新兴的高性能和嵌入式应用，从图像/视频/音频识别到自动翻译和商业分析，以及所有形式的机器人都依赖于机器学习技术。这种趋势甚至开始渗透到我们的社区中，在那里，一个用于评价新应用的套件，PARSEC[2]，其中有一般的评分是采用了机器学习算法[4]。这种应用的趋势伴随着机器学习的第三个显著的趋势，其中基于神经网络（特别是卷积神经网络[27]和深度神经网络[16]〕的少量技术在过去几年中脱颖而出。因此，加速器设计迎来了一个独特的机会，可以实现两个方面的最佳：广泛的应用范围，以及高性能和高效率，主要原因是目标算法只有有限个。
    目前，这些工作大多在GPU[5]，或在FPGA[3]上通过SIMD[41]执行。 然而，有些研究人员已经意识到了上述的趋势，并且提出了用于CNN[3]或MLP[38]的加速器设计；在其他应用领域方面，如图像处理，也提出了针对机器学习原语如卷积的高效实现。还有一些人提出了CNN和定制型机器学习算法的ASIC实现。然而，所有这些工作都只是专注于这些计算原语的高效实现，为了简单起见，却有意的回避内存传输问题，或者直接把加速器计算核心通过一个或多或少精心设计的DMA，挂载在内存上。
    虽然计算原语的高效实现是第一重要的步骤，但是低效的存储传输可能会浪费加速器的吞吐带宽，效能和成本优势，即阿姆达尔定律。所以，就如处理器设计中一样，存储传输应该作为第一考虑因素而不是第二步才加以考虑。与处理器不同的是，人们可以在目标算法中考虑内存传输的特定性质，就像设计计算加速器中所做的一样。这在机器学习的领域中尤其重要，因为目前有一个明显的趋势，为了提供更好的精度和更多的功能，神经网络的规模一直在扩大[16, 26 ]。 
    本研究中，我们研究了一个加速器设计，可以兼容目前最流行的先进的算法，即卷积神经网络（CNNs）和深度神经网络（DNS）。我们专注于加速器的内存使用的设计，我们研究加速器架构和控制，以尽量减少内存传输，并尽可能高效地执行它们。我们提出了一个在65纳米的设计，它可以每.02ns并行执行496个16位定点操作，即452 GOP/s，面积为3.02MM2，功耗85MW不包括主存储器访问）。在10个最近能找到的最大层次的CNNs和DNNs计算实例中，这个加速器比2GHz的128比特SIMD核心平均快117.87x，且拥有21.08x的能效优势（包括主存储器访问）。
    总的来说，我们的主要贡献如下：
    * 为现在流行的最先进机器学习算法，CNN和DNN，设计了一个完整实现（包括place＆Routing）的加速器核心。
    
